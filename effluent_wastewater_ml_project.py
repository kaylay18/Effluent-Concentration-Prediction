# -*- coding: utf-8 -*-
"""Effluent Wastewater ML Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kynSKgczK_TUI31JZF3S6vayeE-gSoWS
"""

!pip install scikit-optimize

!pip install streamlit

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
import shap
from skopt import BayesSearchCV, gp_minimize
from skopt.space import Real, Integer
import math
import pickle
import seaborn as sns
import matplotlib.pyplot as plt
import streamlit as st

# Load the dataset
file_name = 'Wastewater Effluent Dataset.xlsx'
df = pd.read_excel(file_name)
df.head()

# Check for missing data
missing_data = df.isnull().sum()

# Print the results
print(missing_data)

# Fill missing values with the mean of each column
for column in df.columns:
    if df[column].isnull().any():  # Check if column has missing values
        mean_value = df[column].mean()  # Calculate mean of the column
        df[column].fillna(mean_value, inplace=True)  # Fill missing values with the mean

# Verify if there are any missing values left
print(df.isnull().sum())

# Calculate the IQR for each column
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1

# Identify outliers
outliers = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))

# Remove outliers
df_cleaned = df[~outliers.any(axis=1)]

df_cleaned.describe()

# Calculate the correlation matrix
correlation_matrix = df_cleaned.corr()

# Create a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

# Splitting the dataset into training and test sets
columns_to_drop = [col for col in df_cleaned.columns if 'EC_eff' in col]
X = df_cleaned.drop(columns=columns_to_drop, axis=1)
y = df_cleaned['EC_eff']

# Split data into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardizing the data (for certain models like SVM)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Machine Learning Models
models = {
    "Random Forest": RandomForestClassifier(random_state=42),
    "Logistic Regression": LogisticRegression(random_state=42, max_iter=500),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "SVM": SVC(random_state=42),
    "KNN": KNeighborsRegressor(n_neighbors=5)
}

# Training and evaluating the models
best_model = None
best_rmse = float('inf')

for model_name, model in models.items():
    print(f"Training {model_name}...")

    # Use scaled data for SVM, for others, use non-scaled data
    if model_name in ["SVM", "KNN"]:
        model.fit(X_train_scaled, y_train)
        y_test_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_test_pred = model.predict(X_test)

    # Calculate evaluation metrics
    rmse_test = math.sqrt(mean_squared_error(y_test, y_test_pred))
    mae_test = mean_absolute_error(y_test, y_test_pred)
    r2_test = r2_score(y_test, y_test_pred)

    # Print the metrics for the current model
    print(f"{model_name} Metrics:")
    print(f"  RMSE: {rmse_test:.4f}")
    print(f"  MAE: {mae_test:.4f}")
    print(f"  R^2: {r2_test:.4f}")

    # Save the best model based on RMSE for the test dataset
    if rmse_test < best_rmse:
        best_rmse = rmse_test
        best_model = model
        best_model_name = model_name

    print(f"{model_name} RMSE on Test Data: {rmse_test}")

# Save the best model
with open('best_model.pkl', 'wb') as f:
    pickle.dump(best_model, f)

print(f"Best model: {best_model_name} with RMSE: {best_rmse}")

# 1. Grid Search
param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]
}

grid_search = GridSearchCV(KNeighborsRegressor(),
                           param_grid, scoring='neg_mean_squared_error', cv=5)
grid_search.fit(X_train, y_train)

# 2. Random Search
param_dist = {
    'n_neighbors': np.arange(1, 21),
    'weights': ['uniform', 'distance'],
    'p': [1, 2]
}

random_search = RandomizedSearchCV(KNeighborsRegressor(),
                                   param_dist, n_iter=10, scoring='neg_mean_squared_error', cv=5)
random_search.fit(X_train, y_train)

# 3. Bayesian Optimization
search_spaces = {
    'n_neighbors': (1, 20),  # Adjust range for KNN
    'weights': ['uniform', 'distance'],
    'p': [1, 2]
}

bayes_search = RandomizedSearchCV(KNeighborsRegressor(),  # Use RandomizedSearchCV for Bayesian Optimization (common practice)
                             search_spaces, n_iter=10, scoring='neg_mean_squared_error', cv=5)
bayes_search.fit(X_train, y_train)
# Evaluation
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    return rmse, r2, mae

# Evaluate each method
for model_name, model in [('Grid Search', grid_search.best_estimator_),
                         ('Random Search', random_search.best_estimator_),
                         ('Bayesian Optimization', bayes_search.best_estimator_)]:
    rmse, r2, mae = evaluate_model(model, X_test, y_test)
    print(f"{model_name}:")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  R^2: {r2:.4f}")
    print(f"  MAE: {mae:.4f}")

results = {}
for model_name, model in [('Grid Search', grid_search.best_estimator_),
                         ('Random Search', random_search.best_estimator_),
                         ('Bayesian Optimization', bayes_search.best_estimator_)]:
    rmse, r2, mae = evaluate_model(model, X_test, y_test)
    results[model_name] = {'RMSE': rmse, 'R^2': r2, 'MAE': mae}

# Find the best method based on lowest RMSE
best_method = min(results, key=lambda k: results[k]['RMSE'])

# Print the best method
print(f"The best hyperparameter tuning method is: {best_method}")
print(f"with RMSE: {results[best_method]['RMSE']:.4f}, "
      f"R^2: {results[best_method]['R^2']:.4f}, "
      f"and MAE: {results[best_method]['MAE']:.4f}")

# Define the parameter grid for Grid Search
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30]
}

# Create a Grid Search object
grid_search = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid,
    scoring='neg_mean_squared_error',
    cv=5,
    n_jobs=-1,  # Use all available cores for parallel processing
    return_train_score=True
)

# Fit the Grid Search to the data
grid_search.fit(X_train, y_train)

# Extract results
results = grid_search.cv_results_

# Create a meshgrid of hyperparameter values
n_estimators_values = param_grid['n_estimators']
max_depth_values = param_grid['max_depth']
N_ESTIMATORS, MAX_DEPTH = np.meshgrid(n_estimators_values, max_depth_values)

# Calculate RMSE for each point on the meshgrid
rmse_values = np.zeros_like(N_ESTIMATORS, dtype=float)
for i, n_estimators in enumerate(n_estimators_values):
    for j, max_depth in enumerate(max_depth_values):
        # Find the corresponding result in the Grid Search results
        mask = (results['param_n_estimators'] == n_estimators) & (results['param_max_depth'] == max_depth)
        rmse_values[j, i] = np.sqrt(-results['mean_test_score'][mask][0])

# Create the contour plot
plt.figure(figsize=(10, 8))
contour = plt.contourf(N_ESTIMATORS, MAX_DEPTH, rmse_values, levels=10, cmap='jet')
plt.xlim(100, 300)  # Set x-axis limits
plt.ylim(10, 30)    # Set y-axis limits
plt.colorbar(contour, label='RMSE')
plt.xlabel('Number of Estimators (n_estimators)')
plt.ylabel('Maximum Depth (max_depth)')
plt.title('Contour Plot of RMSE for Grid Search')
plt.show()

# Partial Dependence Plot
knn_model = KNeighborsRegressor(n_neighbors=5)
knn_model.fit(X_train, y_train)

# Create a SHAP explainer
explainer = shap.KernelExplainer(knn_model.predict, X_train)

# Calculate SHAP values
shap_values = explainer.shap_values(X_train)

# Plot the PDPs
shap.partial_dependence_plot(
    "EC_in", knn_model.predict, X_train
)

# Beeswarm Plot
model = KNeighborsRegressor()  # Replace with your model
model.fit(X_train, y_train)

# Create a SHAP explainer (consider using KernelExplainer for large datasets)
explainer = shap.KernelExplainer(model.predict, X_train[:100])  # Sample 100 data points

# Calculate SHAP values for a subset of data
shap_values = explainer.shap_values(X_test[:100])

# Create a beeswarm plot
shap.summary_plot(shap_values, X_test[:100])

# Title of the app
st.title("Wastewater Effluent ConcentrationPrediction")

# Sidebar inputs for user preferences
st.sidebar.header("User Preferences")

influent_flowrate = st.number_input("Enter the influent Flowrate (L/h):", value=40000)
influent_zinc = st.number_input("Enter concentration of zinc in the influent (%):", min_value=0, max_value=100, value=0)
influent_ph = st.number_input("Enter pH of the influent:", min_value=0, max_value=14, value=7)
influent_BOD = st.number_input("Enter influent Biochemical Oxygen Demand (mg/L):", value=350)
influent_COD = st.number_input("Enter influent Chemical Oxygen Demand (mg/L):", value=350)
influent_TSS = st.number_input("Enter Total Suspended Solids in influent (mg/L)", value=100)
influent_VSS = st.number_input("Enter Volitile Suspended Solids in influent (mg/L):", value=50)
influent_EC = st.number_input("Enter influent Electrical Conductivity (S/m):", value=2000)

# Encoding the inputs manually (same encoding as in your training data)
input_data = pd.DataFrame({
    'Q_in': [influent_flowrate],
    'Zn_in': [influent_zinc],
    'pH_in': [influent_ph],
    'BOD_in': [influent_BOD],
    'COD_in': [influent_COD],
    'TSS_in': [influent_TSS],
    'VSS_in': [influent_VSS],
    'EC_in': [influent_EC]

})

# One-hot encode the input data (ensure it matches the training data)
input_encoded = pd.get_dummies(input_data)

# Align columns with the training data (required columns)
required_columns = model.feature_names_in_  # Get the feature columns from the model
for col in required_columns:
    if col not in input_encoded.columns:
        input_encoded[col] = 0
input_encoded = input_encoded[required_columns]

# Make the prediction
prediction = model.predict(input_encoded)[0]

# Display the prediction
st.subheader(f"Electrical Coductivity of the Effluent: {prediction}")